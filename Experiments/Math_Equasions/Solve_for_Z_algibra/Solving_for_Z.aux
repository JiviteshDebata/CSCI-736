\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Given:}{1}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Given Neural Net}}{1}{}\protected@file@percent }
\newlabel{fig:NeuralNet}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Solve $z_1 = z_2$ for $x_1$}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Explicitly, the idea going foward is to convert this into a system of differential equasions. Then use numerical meathods to get a "close enough" solution that can be computed in "reasonable" time. Equ. 69 is directly related to the variable B. Whatever method used to aproxomate B can be used again for the components of B. Since B is not the variables we care about, this proccess will be generalizable, and SHOULD scale to "deaper" equasions (This translates to allowing one to solve arbitrarily deap NN). It is also interesting to note that $x_1$ and $x_2$ are not dependent variables. They are independent since the value of input neuron 1 is not related to the input neuron 2 value. This impleis that for tasks such as image detection and image processing where 1 pixel is not related to the value of another pixel will be inherintly different than other tasks such as forcast predictions (where sunlight, windspeed, terrane, etc. are all dependent on each other). For our purposes, having $x_1$ and $x_2$ be indepenent means that they can be decoupled allsowing for simpler ODE meathods. Being able to use simplar ODE methods means this will be be scaleable to $x_n$.}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{TLDR: By solving for B instead of directly for $x_1, x_2$ than there is a method for solving Deep NN's. Having $x_1, x_2$ be independent variables than we can solve for an arbitrary number of inputs.\newline  }{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusion}{19}{}\protected@file@percent }
\gdef \@abspage@last{19}
